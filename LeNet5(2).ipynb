{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5213f929-3201-47cf-b4af-efec1789a04a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Époque 1/10 - Perte: 3.4965 | Validation: 3.4965 | Acc: 0.0300 / 0.0000\n",
      "Époque 2/10 - Perte: 3.4965 | Validation: 3.4965 | Acc: 0.0600 / 0.0000\n",
      "Époque 3/10 - Perte: 3.4964 | Validation: 3.4965 | Acc: 0.0600 / 0.0000\n"
     ]
    }
   ],
   "source": [
    "# Importation des bibliothèques nécessaires\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "# --- Fonctions d'activation ---\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_derivative(x):\n",
    "    return np.where(x > 0, 1, 0)\n",
    "\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))  # stabilité numérique\n",
    "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "\n",
    "# --- Initialisation des poids et biais ---\n",
    "def initialize_weights():\n",
    "    weights = {\n",
    "        'C1': np.random.randn(6, 1, 5, 5) * 0.01,     # C1 : 6 filtres 5x5\n",
    "        'C3': np.random.randn(16, 6, 5, 5) * 0.01,    # C3 : 16 filtres\n",
    "        'C5': np.random.randn(120, 16, 5, 5) * 0.01,  # C5 : 120 filtres\n",
    "        'F6': np.random.randn(120, 84) * 0.01,        # F6 : fully connected (120 -> 84)\n",
    "        'out': np.random.randn(84, 33) * 0.01         # Sortie : 33 classes\n",
    "    }\n",
    "    biases = {\n",
    "        'C1': np.zeros((6, 1)),\n",
    "        'C3': np.zeros((16, 1)),\n",
    "        'C5': np.zeros((120, 1)),\n",
    "        'F6': np.zeros((84, 1)),\n",
    "        'out': np.zeros((33, 1))\n",
    "    }\n",
    "    return weights, biases\n",
    "\n",
    "# --- Convolution manuelle ---\n",
    "def conv2d(input, filters, bias, stride=1, padding=0):\n",
    "    n_filters, d_filter, h_filter, w_filter = filters.shape\n",
    "    n, d, h, w = input.shape\n",
    "    h_out = (h - h_filter + 2 * padding) // stride + 1\n",
    "    w_out = (w - w_filter + 2 * padding) // stride + 1\n",
    "    output = np.zeros((n, n_filters, h_out, w_out))\n",
    "    \n",
    "    # Ajout de padding si nécessaire\n",
    "    input_padded = np.pad(input, ((0, 0), (0, 0), (padding, padding), (padding, padding)), mode='constant') if padding > 0 else input\n",
    "\n",
    "    # Calcul de la convolution\n",
    "    for i in range(n):\n",
    "        for f in range(n_filters):\n",
    "            for j in range(h_out):\n",
    "                for k in range(w_out):\n",
    "                    region = input_padded[i, :, j*stride:j*stride+h_filter, k*stride:k*stride+w_filter]\n",
    "                    output[i, f, j, k] = np.sum(region * filters[f] + bias[f])\n",
    "    return output\n",
    "\n",
    "# --- Pooling moyen (average pooling) ---\n",
    "def avg_pooling(input, pool_size=2, stride=2):\n",
    "    n, d, h, w = input.shape\n",
    "    h_out = (h - pool_size) // stride + 1\n",
    "    w_out = (w - pool_size) // stride + 1\n",
    "    output = np.zeros((n, d, h_out, w_out))\n",
    "    \n",
    "    for i in range(n):\n",
    "        for c in range(d):\n",
    "            for j in range(h_out):\n",
    "                for k in range(w_out):\n",
    "                    output[i, c, j, k] = np.mean(input[i, c, j*stride:j*stride+pool_size, k*stride:k*stride+pool_size])\n",
    "    return output\n",
    "\n",
    "# --- Propagation avant (forward pass) ---\n",
    "def forward(X, weights, biases):\n",
    "    C1 = relu(conv2d(X, weights['C1'], biases['C1']))\n",
    "    S2 = avg_pooling(C1)\n",
    "    C3 = relu(conv2d(S2, weights['C3'], biases['C3']))\n",
    "    S4 = avg_pooling(C3)\n",
    "    C5 = relu(conv2d(S4, weights['C5'], biases['C5']))\n",
    "    C5_flat = C5.reshape(C5.shape[0], -1)\n",
    "    F6 = relu(np.dot(C5_flat, weights['F6']) + biases['F6'].T)\n",
    "    out = np.dot(F6, weights['out']) + biases['out'].T\n",
    "    probs = softmax(out)\n",
    "    return probs, {'C1': C1, 'S2': S2, 'C3': C3, 'S4': S4, 'C5': C5, 'F6': F6, 'out': out}\n",
    "\n",
    "# --- Fonction de coût (Cross-Entropy) ---\n",
    "def cross_entropy_loss(probs, y):\n",
    "    n = y.shape[0]\n",
    "    log_probs = -np.log(probs[range(n), y])\n",
    "    return np.mean(log_probs)\n",
    "\n",
    "# --- Rétropropagation (partielle : couches fully connected uniquement) ---\n",
    "def backward(X, y, probs, cache, weights, biases, lr=0.01):\n",
    "    n = X.shape[0]\n",
    "    d_out = probs.copy()\n",
    "    d_out[range(n), y] -= 1\n",
    "    d_out /= n\n",
    "\n",
    "    dW_out = np.dot(cache['F6'].T, d_out)\n",
    "    db_out = np.sum(d_out, axis=0, keepdims=True).T\n",
    "    dF6 = np.dot(d_out, weights['out'].T) * relu_derivative(cache['F6'])\n",
    "\n",
    "    dW_F6 = np.dot(cache['C5'].reshape(n, -1).T, dF6)\n",
    "    db_F6 = np.sum(dF6, axis=0, keepdims=True).T\n",
    "\n",
    "    weights['out'] -= lr * dW_out\n",
    "    biases['out'] -= lr * db_out\n",
    "    weights['F6'] -= lr * dW_F6\n",
    "    biases['F6'] -= lr * db_F6\n",
    "\n",
    "    return weights, biases\n",
    "\n",
    "# --- Entraînement du modèle ---\n",
    "def train(X_train, y_train, X_val, y_val, weights, biases, epochs=10, lr=0.01):\n",
    "    train_losses, val_losses = [], []\n",
    "    train_accs, val_accs = [], []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        probs, cache = forward(X_train, weights, biases)\n",
    "        train_loss = cross_entropy_loss(probs, y_train)\n",
    "        train_acc = np.mean(np.argmax(probs, axis=1) == y_train)\n",
    "\n",
    "        val_probs, _ = forward(X_val, weights, biases)\n",
    "        val_loss = cross_entropy_loss(val_probs, y_val)\n",
    "        val_acc = np.mean(np.argmax(val_probs, axis=1) == y_val)\n",
    "\n",
    "        weights, biases = backward(X_train, y_train, probs, cache, weights, biases, lr)\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        train_accs.append(train_acc)\n",
    "        val_accs.append(val_acc)\n",
    "\n",
    "        print(f\"Époque {epoch+1}/{epochs} - Perte: {train_loss:.4f} | Validation: {val_loss:.4f} | Acc: {train_acc:.4f} / {val_acc:.4f}\")\n",
    "\n",
    "    return weights, biases, train_losses, val_losses, train_accs, val_accs\n",
    "\n",
    "# --- Visualisation des résultats ---\n",
    "def plot_curves(train_losses, val_losses, train_accs, val_accs):\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_losses, label='Perte entraînement')\n",
    "    plt.plot(val_losses, label='Perte validation')\n",
    "    plt.legend()\n",
    "    plt.title('Courbe de perte')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(train_accs, label='Précision entraînement')\n",
    "    plt.plot(val_accs, label='Précision validation')\n",
    "    plt.legend()\n",
    "    plt.title('Courbe de précision')\n",
    "    plt.show()\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.xlabel(\"Prédit\")\n",
    "    plt.ylabel(\"Vérité\")\n",
    "    plt.title(\"Matrice de confusion\")\n",
    "    plt.show()\n",
    "\n",
    "def plot_feature_maps(feature_maps, layer_name, num_filters=6):\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    for i in range(min(num_filters, feature_maps.shape[1])):\n",
    "        plt.subplot(1, num_filters, i+1)\n",
    "        plt.imshow(feature_maps[0, i], cmap='gray')\n",
    "        plt.axis('off')\n",
    "        plt.title(f'{layer_name} Filtre {i+1}')\n",
    "    plt.show()\n",
    "\n",
    "# --- Simulation des données ---\n",
    "X_train = np.random.randn(100, 1, 32, 32)\n",
    "y_train = np.random.randint(0, 33, 100)\n",
    "X_val = np.random.randn(20, 1, 32, 32)\n",
    "y_val = np.random.randint(0, 33, 20)\n",
    "\n",
    "# --- Exécution ---\n",
    "weights, biases = initialize_weights()\n",
    "weights, biases, train_losses, val_losses, train_accs, val_accs = train(X_train, y_train, X_val, y_val, weights, biases)\n",
    "\n",
    "plot_curves(train_losses, val_losses, train_accs, val_accs)\n",
    "\n",
    "probs, _ = forward(X_val, weights, biases)\n",
    "y_pred = np.argmax(probs, axis=1)\n",
    "plot_confusion_matrix(y_val, y_pred)\n",
    "\n",
    "_, cache = forward(X_val[:1], weights, biases)\n",
    "plot_feature_maps(cache['C1'], 'C1', num_filters=6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb44d753-5a2c-4dd1-8d36-ad314b811fcb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
